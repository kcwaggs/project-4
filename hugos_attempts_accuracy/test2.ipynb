{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e1e0244-465a-4749-8000-dc41dc2c9fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'resources/feature_engineered_music_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d01a9f-5bec-45dd-8d8f-3e2946b703d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in dataset:\n",
      " is_playable                       0\n",
      "danceability                      0\n",
      "energy                            0\n",
      "key                               0\n",
      "loudness                          0\n",
      "                                 ..\n",
      "main_genre_R&B/Soul/Funk          0\n",
      "main_genre_Rock                   0\n",
      "main_genre_Seasonal/Holiday       0\n",
      "main_genre_Soundtrack/Theme       0\n",
      "main_genre_World/International    0\n",
      "Length: 96, dtype: int64\n",
      "Infinite values in dataset:\n",
      " is_playable                       True\n",
      "danceability                      True\n",
      "energy                            True\n",
      "key                               True\n",
      "loudness                          True\n",
      "                                  ... \n",
      "main_genre_R&B/Soul/Funk          True\n",
      "main_genre_Rock                   True\n",
      "main_genre_Seasonal/Holiday       True\n",
      "main_genre_Soundtrack/Theme       True\n",
      "main_genre_World/International    True\n",
      "Length: 96, dtype: bool\n",
      "Target variable distribution:\n",
      " count    226901.000000\n",
      "mean          3.363158\n",
      "std           0.713261\n",
      "min           1.098612\n",
      "25%           3.044522\n",
      "50%           3.555348\n",
      "75%           3.871201\n",
      "max           4.615121\n",
      "Name: popularity_log, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Drop non-numeric columns\n",
    "non_numeric_columns = ['track_uri', 'name', 'artists_names', 'artists_uris', 'playlist_uris', 'analysis_url', 'artists_genres']\n",
    "df.drop(columns=non_numeric_columns, inplace=True)\n",
    "\n",
    "# One-hot encode remaining non-numeric columns if any\n",
    "remaining_non_numeric_columns = df.select_dtypes(include=['object']).columns\n",
    "df = pd.get_dummies(df, columns=remaining_non_numeric_columns, drop_first=True)\n",
    "\n",
    "# Check for NaN and infinite values\n",
    "print(\"NaN values in dataset:\\n\", df.isna().sum())\n",
    "print(\"Infinite values in dataset:\\n\", np.isfinite(df).all())\n",
    "\n",
    "# Handle NaN values if any\n",
    "df.fillna(df.mean(), inplace=True)  # or other imputation method\n",
    "\n",
    "# Replace infinite values\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Define X (features) and y (target)\n",
    "X_features = df.drop(columns=['popularity_log'])  # Use the appropriate target column name\n",
    "y_popularity = df['popularity_log']  # Use the appropriate target column name\n",
    "\n",
    "# Check the distribution of the target variable\n",
    "print(\"Target variable distribution:\\n\", y_popularity.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c8af490-fe16-47db-8700-6c0702046fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finite values in scaled training data: True\n",
      "Finite values in scaled test data: True\n"
     ]
    }
   ],
   "source": [
    "# Use sklearn to split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_popularity, random_state=78, test_size=0.2)\n",
    "\n",
    "# Create scaler instance\n",
    "X_scaler = skl.preprocessing.StandardScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "# Verify scaling\n",
    "print(\"Finite values in scaled training data:\", np.isfinite(X_train_scaled).all())\n",
    "print(\"Finite values in scaled test data:\", np.isfinite(X_test_scaled).all())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b79f4c2-6ce8-41ad-b40a-67cfeae8660a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m5673/5673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 0.7825 - mean_squared_error: 0.7825 - val_loss: 0.3324 - val_mean_squared_error: 0.3324\n",
      "Epoch 2/10\n",
      "\u001b[1m5673/5673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 0.3237 - mean_squared_error: 0.3237 - val_loss: 0.3237 - val_mean_squared_error: 0.3237\n",
      "Epoch 3/10\n",
      "\u001b[1m5673/5673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 0.3147 - mean_squared_error: 0.3147 - val_loss: 0.3160 - val_mean_squared_error: 0.3160\n",
      "Epoch 4/10\n",
      "\u001b[1m5673/5673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 0.3115 - mean_squared_error: 0.3115 - val_loss: 0.3132 - val_mean_squared_error: 0.3132\n",
      "Epoch 5/10\n",
      "\u001b[1m5673/5673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 994us/step - loss: 0.3085 - mean_squared_error: 0.3085 - val_loss: 0.3128 - val_mean_squared_error: 0.3128\n",
      "Epoch 6/10\n",
      "\u001b[1m5673/5673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 997us/step - loss: 0.3059 - mean_squared_error: 0.3059 - val_loss: 0.3115 - val_mean_squared_error: 0.3115\n",
      "Epoch 7/10\n",
      "\u001b[1m5673/5673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 0.3046 - mean_squared_error: 0.3046 - val_loss: 0.3132 - val_mean_squared_error: 0.3132\n",
      "Epoch 8/10\n",
      "\u001b[1m5673/5673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 964us/step - loss: 0.3043 - mean_squared_error: 0.3043 - val_loss: 0.3097 - val_mean_squared_error: 0.3097\n",
      "Epoch 9/10\n",
      "\u001b[1m5673/5673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 0.3055 - mean_squared_error: 0.3055 - val_loss: 0.3110 - val_mean_squared_error: 0.3110\n",
      "Epoch 10/10\n",
      "\u001b[1m5673/5673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 0.3039 - mean_squared_error: 0.3039 - val_loss: 0.3090 - val_mean_squared_error: 0.3090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x27f995e1700>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a simple model to start with\n",
    "def create_simple_model():\n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    nn_model.add(tf.keras.layers.Input(shape=(X_train_scaled.shape[1],)))\n",
    "    \n",
    "    # Single hidden layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units=16, activation='relu', kernel_initializer='he_normal'))\n",
    "    \n",
    "    nn_model.add(tf.keras.layers.Dense(units=1))\n",
    "\n",
    "    # Compile the model with Adam optimizer\n",
    "    nn_model.compile(loss=\"mean_squared_error\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[\"mean_squared_error\"])\n",
    "    \n",
    "    return nn_model\n",
    "\n",
    "# Create and train the simple model\n",
    "simple_model = create_simple_model()\n",
    "simple_model.fit(X_train_scaled, y_train, epochs=10, validation_data=(X_test_scaled, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bdd80f4-c937-4494-88e4-6d249dd4c4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1419/1419 - 1s - 665us/step - loss: 0.3090 - mean_squared_error: 0.3090\n",
      "Simple Model Loss: 0.3090307414531708, Mean Squared Error: 0.3090307414531708\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the simple model\n",
    "model_loss, model_accuracy = simple_model.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "print(f\"Simple Model Loss: {model_loss}, Mean Squared Error: {model_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0b6f370-e66d-4ebe-b5c2-35c8d69a2ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52a8a7e7-bf05-49f2-b582-dfa821aa562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a method that creates a new Sequential model with hyperparameter options\n",
    "def create_model(hp):\n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    nn_model.add(tf.keras.layers.Input(shape=(X_train_scaled.shape[1],)))\n",
    "    \n",
    "    # Allow kerastuner to decide which activation function to use in hidden layers\n",
    "    activation = hp.Choice('activation', ['relu', 'tanh'])\n",
    "    \n",
    "    # Allow kerastuner to decide number of neurons in first layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',\n",
    "        min_value=8,\n",
    "        max_value=16,\n",
    "        step=4), activation=activation, kernel_initializer='he_normal'))\n",
    "\n",
    "    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers\n",
    "    for i in range(hp.Int('num_layers', 1, 2)):\n",
    "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
    "            min_value=8,\n",
    "            max_value=16,\n",
    "            step=4),\n",
    "            activation=activation, kernel_initializer='he_normal'))\n",
    "    \n",
    "    nn_model.add(tf.keras.layers.Dense(units=1))\n",
    "\n",
    "    # Compile the model with Adam optimizer\n",
    "    nn_model.compile(loss=\"mean_squared_error\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[\"mean_squared_error\"])\n",
    "    \n",
    "    return nn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb29ab4-db34-4cfa-b754-252a9c787295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 Complete [00h 00m 34s]\n",
      "val_mean_squared_error: 0.3084581792354584\n",
      "\n",
      "Best val_mean_squared_error So Far: 0.30634915828704834\n",
      "Total elapsed time: 00h 04m 29s\n",
      "\n",
      "Search: Running Trial #10\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "tanh              |tanh              |activation\n",
      "8                 |16                |first_units\n",
      "2                 |2                 |num_layers\n",
      "8                 |12                |units_0\n",
      "12                |8                 |units_1\n",
      "3                 |3                 |tuner/epochs\n",
      "0                 |0                 |tuner/initial_epoch\n",
      "2                 |2                 |tuner/bracket\n",
      "0                 |0                 |tuner/round\n",
      "\n",
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "# Import the kerastuner library\n",
    "tuner = kt.Hyperband(\n",
    "    create_model,\n",
    "    objective=\"val_mean_squared_error\",\n",
    "    max_epochs=20,\n",
    "    hyperband_iterations=2)\n",
    "\n",
    "# Run the kerastuner search for best hyperparameters\n",
    "tuner.search(X_train_scaled, y_train, epochs=10, validation_data=(X_test_scaled, y_test), callbacks=[callback])\n",
    "\n",
    "# Get best model hyperparameters\n",
    "best_hyper = tuner.get_best_hyperparameters(1)[0]\n",
    "print(best_hyper.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a49865-3beb-4288-a616-437e8e0be89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize the best model with best hyperparameters\n",
    "def create_best_model(hp):\n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    nn_model.add(tf.keras.layers.Input(shape=(X_train_scaled.shape[1],)))\n",
    "    \n",
    "    # Hidden layers with best hyperparameters\n",
    "    activation = hp['activation']\n",
    "    nn_model.add(tf.keras.layers.Dense(units=hp['first_units'], activation=activation, kernel_initializer='he_normal'))\n",
    "    for i in range(hp['num_layers']):\n",
    "        nn_model.add(tf.keras.layers.Dense(units=hp[f'units_{i}'], activation=activation, kernel_initializer='he_normal'))\n",
    "    \n",
    "    nn_model.add(tf.keras.layers.Dense(units=1))\n",
    "\n",
    "    # Compile the model with Adam optimizer\n",
    "    nn_model.compile(loss=\"mean_squared_error\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[\"mean_squared_error\"])\n",
    "    \n",
    "    return nn_model\n",
    "\n",
    "# Create the best model with the best hyperparameters\n",
    "best_model = create_best_model(best_hyper)\n",
    "best_model.fit(X_train_scaled, y_train, epochs=10, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the best model\n",
    "model_loss, model_accuracy = best_model.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "print(f\"Tuned Model Loss: {model_loss}, Mean Squared Error: {model_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6a7bd5-c9a6-43b7-a37a-98df64dfb326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict the popularity on the test set\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Scatter plot of actual vs. predicted values\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.xlabel('Actual Popularity')\n",
    "plt.ylabel('Predicted Popularity')\n",
    "plt.title('Actual vs. Predicted Popularity')\n",
    "plt.show()\n",
    "\n",
    "# Calculate additional metrics\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008eed20-829a-4c22-89d3-2138bff637d8",
   "metadata": {},
   "source": [
    "My model has a Mean Absolute Error (MAE) of about 0.396, meaning its predictions are, on average, 0.396 units off from the actual popularity. The R-squared value is around 0.42, indicating that the model explains 42% of the variation in the actual popularity. This shows a moderate level of accuracy: the model captures some trends but is not highly precise. The scatter plot reveals that while predictions generally follow the actual values, there's a significant spread, suggesting variability in predictions. To improve, you could refine features, try more complex models, fine-tune parameters, handle outliers better, and use cross-validation for more robust results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
